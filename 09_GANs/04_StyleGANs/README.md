# **Style Generative Adversarial Networks (StyleGANs)**

## **1. Introduction to StyleGANs**

### **1.1. Lacking Control Over Synthesized Images**

Generative adversarial networks are effective at generating high-quality and large-resolution synthetic images.

The generator model takes as input a point from latent space and generates an image. This model is trained by a second model, called the discriminator, that learns to differentiate real images from the training dataset from fake images generated by the generator model. As such, the two models compete in an adversarial game and find a balance or equilibrium during the training process.

Many improvements to the GAN architecture have been achieved through enhancements to the discriminator model. These changes are motivated by the idea that a better discriminator model will, in turn, lead to the generation of more realistic synthetic images.

As such, the generator has been somewhat neglected and remains a black box. For example, the source of randomness used in the generation of synthetic images is not well understood, including both the amount of randomness in the sampled points and the structure of the latent space.

This limited understanding of the generator is perhaps most exemplified by the general lack of control over the generated images. There are few tools to control the properties of generated images, e.g. the style. This includes high-level features such as background and foreground, and fine-grained details such as the features of synthesized objects or subjects.

This requires both disentangling features or properties in images and adding controls for these properties to the generator model.

### **1.2. StyleGANs - Control Style Using New Generator Model**

StyleGAN is a type of GAN that allows for generating high-resolution, photorealistic images. It was introduced in the paper "StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks" by Karras et al. in 2019. It gives control over the disentangled style properties of generated images.

The StyleGAN is an extension of the progressive growing GAN which is a training technique where the models grow incrementally from small to large images. StyleGAN extends this approach to achieve superior image synthesis quality.

In addition, the style GAN also changes the architecture of the generator significantly. The StyleGAN generator no longer takes a point from the latent space as input; instead, there are two new sources of randomness used to generate a synthetic image: a standalone mapping network and noise layers.

Stochastic variation is introduced through noise added at each point in the generator model. The noise is added to entire feature maps that allow the model to interpret the style in a fine-grained, per-pixel manner.

This per-block incorporation of style vector and noise allows each block to localize both the interpretation of style and the stochastic variation to a given level of detail.

The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis.

### **1.3. Applications of StyleGAN**

StyleGAN has numerous applications in computer vision, including:

- Image synthesis: StyleGAN can be used to generate high-resolution images of objects, scenes, and characters that do not exist in the real world.
- Image editing: StyleGAN can be used to edit images by changing their style, color palette, or texture.
- Image-to-image translation: StyleGAN can be used to translate an image from one domain to another, such as translating a photo of a cat to a painting.

## **2. Architecture of StyleGAN**

The StyleGAN is described as a progressive growing GAN architecture with 6 modifications. The incremental list of changes to the generator are:

1. Baseline Progressive GAN.
2. Addition of tuning and bilinear upsampling.
3. Addition of mapping network and AdaIN (styles).
4. Removal of latent vector input to generator.
5. Addition of noise to each block.
6. Addition Mixing regularization.

<p align="center">
  <img src="https://machinelearningmastery.com/wp-content/uploads/2019/06/Summary-of-the-StyleGAN-Generator-Model-Architecture.png" >
  <br>
  <i>Summary of the StyleGAN Generator Model Architecture.</i>
</p>

### **2.1. Baseline Progressive GAN**

The StyleGAN generator and discriminator models are based on the Progressive Growing GAN (ProGAN) architecture, which was introduced in the paper "Progressive Growing of GANs for Improved Quality, Stability, and Variation" by Karras et al. in 2018.

ProGANs are a type of GAN that uses a progressive training process to generate high-resolution images. The training process starts with a low-resolution image and gradually increases the resolution until the desired resolution is reached. It consists of 2 step:

- The generator step: the generator is trained to produce a synthetic image that is indistinguishable from a real image.
- The discriminator step: the discriminator is trained to distinguish between real and synthetic images.
- The generator and discriminator are trained simultaneously in an adversarial process.

<p align="center">
  <img src="https://editor.analyticsvidhya.com/uploads/63219pg.png" >
  <br>
  <i>Example of Progressive Adding Layers to Generator and Discriminator Models.</i>
</p>

Here we can see in the above figure that Progressive Growing GAN involves using a generator and discriminator model with the traditional GAN structure and its starts with very small images, such as 4×4 pixels.

During the training process, it systematically adds new blocks of convolutional layers to both the generator model and the discriminator model. This incremental addition of the convolutional layers allows the models to learn coarse-level detail effectively at the beginning and later learn even finer detail, both on the generator and discriminator side.

<p align="center">
  <img src="https://editor.analyticsvidhya.com/uploads/61324Screenshot%202021-05-14%20170122.png" >
  <br>
  <i>Example of Progressive Growing GAN.</i>
</p>

The process of adding a new block of layers involves the usage of *skip connection* as shown in the above figure, it is mainly to connect the new block either to the output of the generator or the input of the discriminator and adding it to the existing output or input layer with a weighting which controls the influence of the new block.

<p align="center">
  <img src="https://images.viblo.asia/8b1ec183-ea83-4898-90af-335524e2af0c.gif" >
  <br>
  <i>Example of Progressive Growing GAN.</i>
</p>

The authors observe that a potential benefit of the ProGAN progressive layers is their ability to control different visual features of the image, if utilized properly. The lower the layer (and the resolution), the coarser the features it affects. The paper divides the features into three types:

- Coarse - resolution of up to 8x8 - affects pose, general hair style, face shape, etc
- Middle - resolution of 16x16 to 32x32 - affects finer facial features, hair style, eyes open/closed, etc.
- Fine - resolution of 64x64 to 1024x1024 - affects color scheme (eye, hair and skin) and micro features.

### **2.2. Addition of Tuning and Bilinear Upsampling**

The authors of paper uses bi-linear sampling instead of nearest neighbor up/down sampling (which was used in previous Baseline Progressive GAN architectures) in both generator and discriminator. They implement this bi-linear sampling by low pass filtering the activation with a separable 2nd order binomial filter after each of the upsampling layer and before each of the downsampling layer.

### **2.3. Addition of Mapping Network and AdaIN (styles)**

The Mapping Network’s goal is to encode the input vector into an intermediate vector whose different elements control different visual features. This is a non-trivial process since the ability to control visual features with the input vector is limited, as it must follow the probability density of the training data.

For example, if images of people with black hair are more common in the dataset, then more input values will be mapped to that feature. As a result, the model isn’t capable of mapping parts of the input (elements in the vector) to features, a phenomenon called features entanglement.

However, by using another neural network the model can generate a vector that doesn’t have to follow the training data distribution and can reduce the correlation between features. Instead of directly providing latent vector to input layer the mapping is used. In this paper, the latent vector (z) of size 512 is mapped to another vector of 512 (w). The mapping function is implemented using 8-layer MLP (8- fully connected layers).

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*6lEwRXKiA8WGRlEc.png" >
  <br>
  <i>The generator with the Mapping Network (in addition to the ProGAN synthesis network)</i>
</p>

The AdaIN (Adaptive Instance Normalization) module transfers the encoded information ⱳ, created by the Mapping Network, into the generated image. The module is added to each resolution level of the Synthesis Network and defines the visual expression of the features in that level:

1. Each channel of the convolution layer output is first normalized to make sure the scaling and shifting of step 3 have the expected effect.
2. The intermediate vector ⱳ is transformed using another fully-connected layer (marked as A) into a scale and bias for each channel.
3. The scale and bias vectors shift each channel of the convolution output, thereby defining the importance of each filter in the convolution. This tuning translates the information from ⱳ to a visual representation.

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*uqn4slMHrFYkFmjS.png" >
  <br>
  <i>The generator’s Adaptive Instance Normalization (AdaIN)</i>
</p>

The input to the AdaIN is y = (ys, yb) which is generated by applying (A) to (w). Through AdaIN, each feature map x is normalized separately, and then scaled and biased using the corresponding scalar components from style y. Thus the dimensional of y is twice the number of feature maps  (x) on that layer. The synthesis network contains 18 convolutional layers 2 for each of the resolutions (4×4 – 1024×1024).

```math
AdaIN(x_i, y) = y_{s,i} \times \frac{x_i - \mu(x_i)}{\sigma(x_i)} + y_{b,i}
```

In the above equation:

- $`x_i`$ is the input feature map.
- $`y`$ is the style vector.
- $`y_{s,i}`$ and $`y_{b,i}`$ are the scaling and bias parameters of the affine transformation.
- $`\mu(x_i)`$ and $`\sigma(x_i)`$ are the mean and standard deviation of the input feature map.

The addition of the new mapping network to the architecture also results in the renaming of the generator model to a “synthesis network.”

### **2.4. Removal of Latent Vector Input to Generator**

Most previous style transfer model uses the random input to create the initial latent code of the generator i.e. ProGAN use for the input of the 4×4 level. However the style-GAN authors concluded that the image generation features are controlled by w and  AdaIN. Therefore they replace the initial input with the constant matrix of 4x4x512. This also contributed to increase in the performance of the network.

The StyleGAN team found that the image features are controlled by ⱳ and the AdaIN, and therefore the initial input can be omitted and replaced by constant values. Though the paper doesn’t explain why it improves performance, a safe assumption is that it reduces feature entanglement — it’s easier for the network to learn only using ⱳ without relying on the entangled input vector.

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*8TIREj1JVUT_IF4W.png" >
  <br>
  <i>The Synthesis Network input is replaced with a constant input</i>
</p>

### **2.5. Addition of Noise to Each Block**

There are many aspects in people’s faces that are small and can be seen as stochastic, such as freckles, exact placement of hairs, wrinkles, features which make the image more realistic and increase the variety of outputs. The common method to insert these small features into GAN images is adding random noise to the input vector. However, in many cases it’s tricky to control the noise effect due to the features entanglement phenomenon that was described above, which leads to other features of the image being affected.

The noise in StyleGAN is added in a similar way to the AdaIN mechanism — A scaled noise is added to each channel before the AdaIN module and changes a bit the visual expression of the features of the resolution level it operates on.

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*GwchALioRMC1xlj7Bh0ZMg.png" >
  <br>
  <i>Adding scaled noise to each resolution level of the synthesis network</i>
</p>

In the above image, A Gaussian noise (represented by B) is added to each of these activation maps before the AdaIN operations. A different sample of noise is generated for each block and is interpreted on the basis of scaling factors of that layer.

### **2.6. Addition Mixing Regularization**

The Style generation used intermediate vector at each level of synthesis network which may cause network to learn correlation between different levels. In order to reduce the correlation, the model randomly selects two input vectors (z1 and z2) and generates the intermediate vector (w1 and w2) for them. It then trains some of the levels with the first and switches (in a random split point) to the other to train the rest of the levels. This switch in random split points ensures that network don’t learn correlation very much.

Though it doesn’t improve the model performance on all datasets, this concept has a very interesting side effect — its ability to combine multiple images in a coherent way.

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PrawB43bcY7JPb6cSB9PcQ.png" >
  <br>
  <i>Style mixing results with different crossover points applied.</i>
</p>

Mixing regularization involves first generating two style vectors from the mapping network. A split point in the synthesis network is chosen and all AdaIN operations prior to the split point use the first style vector and all AdaIN operations after the split point get the second style vector. This encourages the layers and blocks to localize the style to specific parts of the model and corresponding level of detail in the generated image.

Here we can see the impact of the crossover point (different resolutions) on the resulting image:

- Coarse resolutions [4x4–8x8]: eyes/hair/skin color are copied from A whereas the pose, hairstyle, and face shape (which the authors call high-level aspects) are copied from B.
- Middle resolutions [16x16–32x32]: high-level aspects are copied from A and small aspects are copied from B.
- Fine resolutions [64x64–1024x1024]: almost all the style is copied from A and only some color details are copied from B.

## **3. Conclusion**

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*ANwSHXJDmwqjNSxi.png" >
  <br>
  <i>An overview architecture of StyleGAN</i>
</p>
